# Kaggle Competition: Don't Overfit II

This folder contains my notebooks and report for the **Kaggle "Don't Overfit II"** competition.

ðŸ“„ [Full Report (PDF)](./Don't_overfit2_report.pdf)

## Summary
- Explored several ML models: **kNN**, **ANN**, **MLP** and **XGBoost**.  
- Best public score: **0.629**.  
- Key techniques: regularization, cross-validation, and preprocessing.

## Files
- `kNN.ipynb` â€“ k-Nearest Neighbors model with different parameters and custom Approximate Nearest Neighbors approach.
- `XGBoost.ipynb` â€“ Extreme Gradient Boost model.
- `MLP.ipynb` â€“ Neural networks and Muptiple layers perceptron experiments with different width and depth.
- `Report.pdf` â€“ Complete written report.
- `extract_file` â€“ Script to retrieve the older dataset to work with.
